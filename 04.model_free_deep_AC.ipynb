{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c931656b",
      "metadata": {
        "id": "c931656b"
      },
      "source": [
        "# 필수 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1bfc3a3d",
      "metadata": {
        "id": "1bfc3a3d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import collections\n",
        "from torch.distributions import Categorical\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "o_jpjK7NwgbZ"
      },
      "id": "o_jpjK7NwgbZ"
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "learning_rate = 5*1e-5\n",
        "gamma         = 0.99"
      ],
      "metadata": {
        "id": "Drrk1-xewgzJ"
      },
      "id": "Drrk1-xewgzJ",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "829adf61",
      "metadata": {
        "id": "829adf61"
      },
      "source": [
        "# Grid World"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "10223069",
      "metadata": {
        "id": "10223069"
      },
      "outputs": [],
      "source": [
        "# model 설정\n",
        "# 4x4, state는 (x, y), H:hole, G:goal\n",
        "# reward: goal:1 이며 나머지는 0\n",
        "#\n",
        "#     0  1  2  3\n",
        "#     4  H  6  H\n",
        "#     8  9 10  H\n",
        "#     H 12 13  G\n",
        "#\n",
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "\n",
        "    def step(self, a):\n",
        "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
        "        if a == 0:\n",
        "            self.move_left()\n",
        "        elif a == 1:\n",
        "            self.move_down()\n",
        "        elif a == 2:\n",
        "            self.move_right()\n",
        "        elif a == 3:\n",
        "            self.move_up()\n",
        "\n",
        "        if self.x == 3 and self.y == 3:\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_down(self):\n",
        "        self.y += 1\n",
        "        if self.y > 3:\n",
        "            self.y = 3\n",
        "\n",
        "    def move_up(self):\n",
        "        self.y -= 1\n",
        "        if self.y < 0:\n",
        "            self.y = 0\n",
        "\n",
        "    def move_left(self):\n",
        "        self.x -= 1\n",
        "        if self.x < 0:\n",
        "            self.x = 0\n",
        "\n",
        "    def move_right(self):\n",
        "        self.x += 1\n",
        "        if self.x > 3:\n",
        "            self.x = 3\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x == 3 and self.y == 3:\n",
        "            return True\n",
        "        elif self.x == 0 and self.y == 3:\n",
        "            return True\n",
        "        elif self.x == 1 and self.y == 1:\n",
        "            return True\n",
        "        elif self.x == 3 and self.y == 1:\n",
        "            return True\n",
        "        elif self.x == 3 and self.y == 2:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def get_state(self):\n",
        "        return (self.x, self.y)\n",
        "\n",
        "    def reset(self):\n",
        "        # Random Initialize\n",
        "\n",
        "        start = [(0, 0), (1, 0), (2, 0), (3, 0), (0, 1), (2, 1), (2, 0), (2, 1), (2, 2), (3, 1), (3, 2)]\n",
        "        (self.x,self.y)=random.choice(start)\n",
        "        return (self.x, self.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network"
      ],
      "metadata": {
        "id": "au60CQ9Xw6Db"
      },
      "id": "au60CQ9Xw6Db"
    },
    {
      "cell_type": "code",
      "source": [
        "def identity(x):\n",
        "    \"\"\"Return input without any change.\"\"\"\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 output_size,\n",
        "                 output_limit=1.0,\n",
        "                 hidden_sizes=(64, 64),\n",
        "                 activation=F.relu,\n",
        "                 output_activation=identity,\n",
        "                 use_output_layer=True,\n",
        "                 use_actor=False,\n",
        "                 ):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.output_limit = output_limit\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.activation = activation\n",
        "        self.output_activation = output_activation\n",
        "        self.use_output_layer = use_output_layer\n",
        "        self.use_actor = use_actor\n",
        "\n",
        "        # Set hidden layers\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        in_size = self.input_size\n",
        "        for next_size in self.hidden_sizes:\n",
        "            fc = nn.Linear(in_size, next_size)\n",
        "            in_size = next_size\n",
        "            self.hidden_layers.append(fc)\n",
        "\n",
        "        # Set output layers\n",
        "        if self.use_output_layer:\n",
        "            self.output_layer = nn.Linear(in_size, self.output_size)\n",
        "        else:\n",
        "            self.output_layer = identity\n",
        "\n",
        "    def forward(self, x):\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = self.activation(hidden_layer(x))\n",
        "        x = self.output_activation(self.output_layer(x))\n",
        "        # If the network is used as actor network, make sure output is in correct range\n",
        "        x = x * self.output_limit if self.use_actor else x\n",
        "        return x"
      ],
      "metadata": {
        "id": "IcDck0E-w59t"
      },
      "id": "IcDck0E-w59t",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "HeLea9TqxBHh"
      },
      "id": "HeLea9TqxBHh"
    },
    {
      "cell_type": "code",
      "source": [
        "def train(transition,actor,critic,optimizer_actor,optimizer_critic):\n",
        "    log_pi, v, reward, next_obs, done_mask = transition\n",
        "\n",
        "    # Prediction V(s')\n",
        "    next_v = critic(torch.Tensor(next_obs))\n",
        "\n",
        "    # Target for Q regression\n",
        "    q = reward + gamma * done_mask * next_v\n",
        "\n",
        "    # Advantage = Q - V\n",
        "    advant = q - v\n",
        "\n",
        "    # A2C losses\n",
        "    policy_loss = -log_pi * advant.detach()\n",
        "    vf_loss = F.mse_loss(v, q.detach())\n",
        "\n",
        "    # Update value network parameter\n",
        "    optimizer_critic.zero_grad()\n",
        "    vf_loss.backward()\n",
        "    optimizer_critic.step()\n",
        "\n",
        "    # Update policy network parameter\n",
        "    optimizer_actor.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer_actor.step()"
      ],
      "metadata": {
        "id": "r3lBWcJ9xBBf"
      },
      "id": "r3lBWcJ9xBBf",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actor"
      ],
      "metadata": {
        "id": "bXsD4O3pxFXC"
      },
      "id": "bXsD4O3pxFXC"
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(MLP):\n",
        "    def forward(self, x):\n",
        "        x = super(Actor, self).forward(x)\n",
        "        pi = F.softmax(x, dim=-1)\n",
        "\n",
        "        dist = Categorical(pi)\n",
        "        action = dist.sample()\n",
        "        log_pi = dist.log_prob(action)\n",
        "        return action, pi, log_pi"
      ],
      "metadata": {
        "id": "H0LWXtLnxFRy"
      },
      "id": "H0LWXtLnxFRy",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actor Critic"
      ],
      "metadata": {
        "id": "WS0kXaOgyRAK"
      },
      "id": "WS0kXaOgyRAK"
    },
    {
      "cell_type": "code",
      "source": [
        "def AC():\n",
        "    env = GridWorld()\n",
        "    actor = Actor(input_size=2,output_size=4,activation=torch.tanh)\n",
        "    critic = MLP(input_size=2,output_size=1,activation=torch.tanh)\n",
        "\n",
        "    print_interval = 100\n",
        "    score = 0.0\n",
        "    optimizer_actor = optim.Adam(actor.parameters(), lr=learning_rate)\n",
        "    optimizer_critic = optim.Adam(critic.parameters(), lr=learning_rate)\n",
        "\n",
        "    for n_epi in range(10000):\n",
        "        done = False\n",
        "        s = np.asarray(env.reset())\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            a,pi,log_pi = actor(torch.Tensor(s))\n",
        "            v=critic(torch.Tensor(s))\n",
        "            a=a.item()\n",
        "            (x_prime, y_prime), reward, done = env.step(a)\n",
        "            s_prime =  np.array([x_prime, y_prime])\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            s = s_prime\n",
        "            score += reward\n",
        "\n",
        "            transition=[log_pi,v,reward,s_prime,done_mask]\n",
        "            train(transition, actor, critic, optimizer_actor, optimizer_critic)\n",
        "\n",
        "\n",
        "        if n_epi % 100 == 0 and n_epi != 0:\n",
        "            print(\"n_episode :{}, score : {:.1f}\".format(\n",
        "                n_epi, score / print_interval))\n",
        "            score = 0.0\n",
        "\n",
        "    # print policy\n",
        "    for r in range(4):\n",
        "        for c in range(4):\n",
        "            s = np.array([c, r])\n",
        "            _,pi,_ = actor(torch.from_numpy(s).float())\n",
        "            a=pi.argmax()\n",
        "            if c == 3 and r == 3:\n",
        "                str = 'G'\n",
        "            elif c == 0 and r == 3:\n",
        "                str = 'H'\n",
        "            elif c == 1 and r == 1:\n",
        "                str = 'H'\n",
        "            elif c == 3 and r == 1:\n",
        "                str = 'H'\n",
        "            elif c == 3 and r == 2:\n",
        "                str = 'H'\n",
        "            else:\n",
        "                if a == 0:\n",
        "                    str = '<'\n",
        "                elif a == 1:\n",
        "                    str = 'v'\n",
        "                elif a == 2:\n",
        "                    str = '>'\n",
        "                elif a == 3:\n",
        "                    str = '^'\n",
        "            print('%03s  '%str, end='')\n",
        "        print('')"
      ],
      "metadata": {
        "id": "fH44w85xyQ4f"
      },
      "id": "fH44w85xyQ4f",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "pmxp_nwqxKAq"
      },
      "id": "pmxp_nwqxKAq"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    AC()"
      ],
      "metadata": {
        "id": "cB2zq1OLxJ63"
      },
      "id": "cB2zq1OLxJ63",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "04.model_free_deep_AC.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}