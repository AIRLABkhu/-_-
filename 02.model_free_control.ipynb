{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c931656b",
      "metadata": {
        "id": "c931656b"
      },
      "source": [
        "# 필수 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bfc3a3d",
      "metadata": {
        "id": "1bfc3a3d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "829adf61",
      "metadata": {
        "id": "829adf61"
      },
      "source": [
        "# Grid World"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10223069",
      "metadata": {
        "id": "10223069"
      },
      "outputs": [],
      "source": [
        "# model 설정\n",
        "# 4x4, state는 (x, y), H:hole, G:goal\n",
        "# reward: goal:1 이며 나머지는 0\n",
        "#\n",
        "#     0  1  2  3\n",
        "#     4  H  6  H\n",
        "#     8  9 10  H\n",
        "#     H 12 13  G\n",
        "#\n",
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "\n",
        "    def step(self, a):\n",
        "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
        "        if a == 0:\n",
        "            self.move_left()\n",
        "        elif a == 1:\n",
        "            self.move_down()\n",
        "        elif a == 2:\n",
        "            self.move_right()\n",
        "        elif a == 3:\n",
        "            self.move_up()\n",
        "\n",
        "        if self.x == 3 and self.y == 3:\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    # 움직임 정의\n",
        "    def move_down(self):\n",
        "        self.y += 1\n",
        "        if self.y > 3:\n",
        "            self.y = 3\n",
        "\n",
        "    def move_up(self):\n",
        "        self.y -= 1\n",
        "        if self.y < 0:\n",
        "            self.y = 0\n",
        "\n",
        "    def move_left(self):\n",
        "        self.x -= 1\n",
        "        if self.x < 0:\n",
        "            self.x = 0\n",
        "\n",
        "    def move_right(self):\n",
        "        self.x += 1\n",
        "        if self.x > 3:\n",
        "            self.x = 3\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x == 3 and self.y == 3:\n",
        "            return True\n",
        "        elif self.x == 0 and self.y == 3:\n",
        "            return True\n",
        "        elif self.x == 1 and self.y == 1:\n",
        "            return True\n",
        "        elif self.x == 3 and self.y == 1:\n",
        "            return True\n",
        "        elif self.x == 3 and self.y == 2:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def get_state(self):\n",
        "        return (self.x, self.y)\n",
        "\n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f271578d",
      "metadata": {
        "id": "f271578d"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c01695ed",
      "metadata": {
        "id": "c01695ed"
      },
      "outputs": [],
      "source": [
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        self.q_table = np.zeros((4, 4, 4))  # q벨류를 저장하는 변수. 모두 0으로 초기화.\n",
        "        self.eps = 0.8\n",
        "        self.alpha = 0.01\n",
        "        self.ran = np.zeros((1,2))\n",
        "        self.rand2 = np.zeros((1,4))\n",
        "\n",
        "    def print_rands(self):\n",
        "        print(self.ran)\n",
        "        print(self.rand2)\n",
        "\n",
        "    def select_action(self, s):\n",
        "        # eps-greedy로 액션을 선택\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0, 3)\n",
        "            self.ran[0,0] = self.ran[0,0] + 1\n",
        "            self.rand2[0,action] = self.rand2[0,action] + 1\n",
        "        else:\n",
        "            action_val = self.q_table[y, x, :]\n",
        "            action = np.argmax(action_val)\n",
        "            self.ran[0, 1] = self.ran[0, 1] + 1\n",
        "        return action\n",
        "\n",
        "    def update_table_mc(self, history):\n",
        "        # 한 에피소드에 해당하는 history를 입력으로 받아 q 테이블의 값을 업데이트 한다\n",
        "        cum_reward = 0\n",
        "        for transition in history[::-1]:\n",
        "            # print(transition)\n",
        "            s, a, r, s_prime = transition\n",
        "            x, y = s\n",
        "\n",
        "            cum_reward = cum_reward + r\n",
        "\n",
        "            # 몬테 카를로 방식을 이용하여 업데이트.\n",
        "            self.q_table[y, x, a] = self.q_table[y, x, a] + self.alpha * (cum_reward - self.q_table[y, x, a])\n",
        "\n",
        "\n",
        "    def update_table_sarsa(self, transition):\n",
        "        s, a, r, s_prime = transition\n",
        "        x, y = s\n",
        "        next_x, next_y = s_prime\n",
        "        a_prime = self.select_action(s_prime)  # S'에서 선택할 액션 (실제로 취한 액션이 아님)\n",
        "        self.q_table[y, x, a] = self.q_table[y, x, a] + 0.1 * (\n",
        "                    r + self.q_table[next_y, next_x, a_prime] - self.q_table[y, x, a])\n",
        "\n",
        "    def update_table_qlearning(self, transition):\n",
        "        s, a, r, s_prime = transition\n",
        "        x, y = s\n",
        "        next_x, next_y = s_prime\n",
        "        a_prime = self.select_action(s_prime)  # S'에서 선택할 액션 (실제로 취한 액션이 아님)\n",
        "        self.q_table[y, x, a] = self.q_table[y, x, a] + 0.1 * (\n",
        "                    r + np.amax(self.q_table[next_y, next_x, :]) - self.q_table[y, x, a])\n",
        "\n",
        "\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.001\n",
        "        self.eps = max(self.eps, 0.1)\n",
        "\n",
        "    def show_table(self):\n",
        "        # 학습이 각 위치에서 어느 액션의 q 값이 가장 높았는지 보여주는 함수\n",
        "        q_lst = self.q_table.tolist()\n",
        "        data = np.zeros((4, 4))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)\n",
        "\n",
        "    def show_table2(self):\n",
        "        # 학습이 각 위치에서 어느 액션의 q 값이 가장 높았는지 보여주는 함수\n",
        "        max_idx = np.argmax(self.q_table, axis=2)\n",
        "        print(max_idx)\n",
        "        for r in range(4):\n",
        "            for c in range(4):\n",
        "                # print('%d %d'%(r,c))\n",
        "                # print(self.q_table[r,c,:])\n",
        "                if c == 3 and r == 3:\n",
        "                    str = 'G'\n",
        "                elif c == 0 and r == 3:\n",
        "                    str = 'X'\n",
        "                elif c == 1 and r == 1:\n",
        "                    str = 'X'\n",
        "                elif c == 3 and r == 1:\n",
        "                    str = 'X'\n",
        "                elif c == 3 and r == 2:\n",
        "                    str = 'X'\n",
        "                else:\n",
        "                    if max_idx[r][c] == 0:\n",
        "                        str = '<'\n",
        "                    elif max_idx[r][c] == 1:\n",
        "                        str = 'v'\n",
        "                    elif max_idx[r][c] == 2:\n",
        "                        str = '>'\n",
        "                    elif max_idx[r][c] == 3:\n",
        "                        str = '^'\n",
        "                print('%03s  '%str, end='')\n",
        "            print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11a754c1",
      "metadata": {
        "id": "11a754c1"
      },
      "source": [
        "# Monte Carlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46e8d828",
      "metadata": {
        "id": "46e8d828"
      },
      "outputs": [],
      "source": [
        "def mc():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "\n",
        "    for n_epi in range(10000):  # 총 1,000 에피소드 동안 학습\n",
        "        done = False\n",
        "        history = []\n",
        "\n",
        "        s = env.reset()\n",
        "\n",
        "        while not done:  # 한 에피소드가 끝날 때 까지\n",
        "            a = agent.select_action(s)\n",
        "            s_prime, r, done = env.step(a)\n",
        "            history.append((s, a, r, s_prime))\n",
        "            s = s_prime\n",
        "\n",
        "        agent.update_table_mc(history)  # 히스토리를 이용하여 에이전트를 업데이트\n",
        "        agent.anneal_eps()\n",
        "\n",
        "    agent.print_rands()\n",
        "    agent.show_table()  # 학습이 끝난 결과를 출력\n",
        "    agent.show_table2()  # 학습이 끝난 결과를 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe67a5e3",
      "metadata": {
        "id": "fe67a5e3"
      },
      "source": [
        "# SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d6d7465",
      "metadata": {
        "id": "8d6d7465"
      },
      "outputs": [],
      "source": [
        "def sarsa():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "\n",
        "    for n_epi in range(10000):  # 총 1,000 에피소드 동안 학습\n",
        "        done = False\n",
        "        history = []\n",
        "\n",
        "        s = env.reset()\n",
        "\n",
        "        while not done:  # 한 에피소드가 끝날 때 까지\n",
        "            a = agent.select_action(s)\n",
        "            s_prime, r, done = env.step(a)\n",
        "            agent.update_table_sarsa((s,a,r,s_prime))  # 히스토리를 이용하여 에이전트를 업데이트\n",
        "            s = s_prime\n",
        "        agent.anneal_eps()\n",
        "\n",
        "    agent.print_rands()\n",
        "    agent.show_table()  # 학습이 끝난 결과를 출력\n",
        "    agent.show_table2()  # 학습이 끝난 결과를 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fd437e",
      "metadata": {
        "id": "33fd437e"
      },
      "source": [
        "# Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10956fb6",
      "metadata": {
        "id": "10956fb6"
      },
      "outputs": [],
      "source": [
        "def Qlearning():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "\n",
        "    for n_epi in range(10000):  # 총 1,000 에피소드 동안 학습\n",
        "        done = False\n",
        "        history = []\n",
        "\n",
        "        s = env.reset()\n",
        "\n",
        "        while not done:  # 한 에피소드가 끝날 때 까지\n",
        "            a = agent.select_action(s)\n",
        "            s_prime, r, done = env.step(a)\n",
        "            agent.update_table_qlearning((s,a,r,s_prime))  # 히스토리를 이용하여 에이전트를 업데이트\n",
        "            s = s_prime\n",
        "        agent.anneal_eps()\n",
        "\n",
        "    agent.print_rands()\n",
        "    agent.show_table()  # 학습이 끝난 결과를 출력\n",
        "    agent.show_table2()  # 학습이 끝난 결과를 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b1e28f",
      "metadata": {
        "id": "49b1e28f"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b78a9057",
      "metadata": {
        "id": "b78a9057"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    #mc()\n",
        "    #sarsa()\n",
        "    Qlearning()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "02.model_free_control.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}